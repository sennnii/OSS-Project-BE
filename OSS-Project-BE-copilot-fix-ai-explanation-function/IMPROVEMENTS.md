# QMIX 주식 트레이딩 AI 성능 개선 사항

## 문제 분석

기존 모델의 주요 문제점:
1. **Q-value Collapse**: 모든 Q-values가 강하게 음수 (-3.9 ~ -17.4)
2. **학습 실패**: 학습 보상이 계속 음수 유지 (평균 -0.0277)
3. **낮은 백테스트 성능**: 
   - 누적 수익: -0.0071
   - 샤프 비율: -1.494 (음수)
   - 승률: 45.37%
4. **행동 편향**: Hold 행동에 과도하게 편향 (55-61%)

## 근본 원인

1. **보상 스케일 문제**: REWARD_SCALE이 0.01로 너무 작아서 학습 시그널이 약함
2. **과도한 페널티**: 거래 비용과 hold 페널티가 너무 강함
3. **불충분한 탐험**: 너무 빠른 epsilon 감소
4. **보상 클리핑**: -1.0~1.0 클리핑이 학습 시그널 손실

## 개선 사항

### 1. 보상 스케일 대폭 증가 (config.py)
```python
REWARD_SCALE = 100.0  # 0.01 -> 100.0 (10000배 증가)
```
**효과**: Q-value collapse 방지, 강한 학습 시그널 제공

### 2. 보상 함수 개선 (environment.py)
- **실현 수익 강화**: `instant_rewards * 2.0`
- **거래 비용 감소**: `transaction_costs * 0.5`
- **다양성 보너스 증가**: `0.001 -> 0.01`
- **보상 클리핑 제거**: 학습 시그널 보존
- **Hold 페널티 완화**: `-0.0005 -> -0.001` (모든 에이전트가 Hold일 때만)

**효과**: 더 명확한 학습 시그널, 다양한 행동 유도

### 3. 탐험 전략 개선 (config.py)
```python
EPSILON_END = 0.05      # 0.01 -> 0.05 (최소 탐험 유지)
EPSILON_DECAY_STEPS = 150000  # 100000 -> 150000 (50% 증가)
WARMUP_STEPS = 5000     # 새로 추가
```
**효과**: 충분한 탐험으로 더 나은 정책 발견

### 4. Warmup Phase 추가 (main.py, config.py)
- 초기 5000 스텝 동안 랜덤 행동으로 경험 수집
- 학습은 warmup 완료 후 시작
- Target network 업데이트도 warmup 후

**효과**: 다양한 초기 경험으로 안정적인 학습 시작

### 5. 학습 강도 증가 (main.py)
```python
# 초기 50 에피소드: 스텝당 8번 학습
# 이후: 스텝당 4번 학습
num_updates = 8 if i_episode < 50 else 4
```
**효과**: 초기 빠른 학습, 후기 안정적 학습

### 6. 훈련 모니터링 강화 (main.py, qmix_model.py)
- Loss 추적
- 평균 Q-value 추적
- 실시간 출력 개선

**효과**: 학습 진행 상황 가시화, 디버깅 용이

### 7. 에피소드 수 증가 (config.py)
```python
NUM_EPISODES = 200  # 100 -> 200
```
**효과**: 충분한 학습 시간 확보

## 예상 개선 효과

### 학습 성능
- **Q-value 범위**: 음수에서 양수로 전환 예상
- **학습 보상**: 음수에서 양수로 점진적 개선
- **수렴 속도**: warmup과 강한 학습으로 빠른 수렴

### 백테스트 성능
- **누적 수익**: 음수에서 양수로 전환 예상
- **샤프 비율**: 음수에서 양수로 개선 (>0.5 목표)
- **승률**: 45% -> 55%+ 개선 예상
- **행동 다양성**: Hold 편향 감소, 균형잡힌 행동 분포

### 전체적 개선
1. **강한 학습 시그널**: 100배 증가한 보상 스케일
2. **효과적 탐험**: Warmup + 완화된 epsilon
3. **안정적 학습**: 점진적 학습 강도 조절
4. **투명한 모니터링**: Q-value와 Loss 실시간 추적

## 주요 변경 파일

1. **config.py**: 하이퍼파라미터 조정
2. **environment.py**: 보상 함수 개선
3. **main.py**: Warmup phase, 모니터링 추가
4. **qmix_model.py**: 학습 메트릭 반환

## 실행 방법

```bash
python main.py
```

기존과 동일하게 실행하면 자동으로 개선된 설정이 적용됩니다.

## 모니터링 출력 예시

```
Ep 10/200 | Eps: 0.950 | R: 15.32 | Avg: 12.45 | Best: 18.20 | Q: 5.67 | L: 0.0234 | Time: 8.5m
```

- **R**: 에피소드 보상
- **Avg**: 최근 10 에피소드 평균
- **Best**: 최고 보상
- **Q**: 평균 Q-value (양수로 증가 확인)
- **L**: Loss (감소 추세 확인)

## 주의사항

1. **실행 시간**: 200 에피소드 + warmup으로 실행 시간 증가 예상
2. **메모리**: 버퍼 크기 100,000으로 메모리 사용 증가
3. **GPU 권장**: CUDA 사용 시 학습 속도 대폭 향상

## 추가 개선 가능 사항

향후 고려할 수 있는 개선:
1. Prioritized Experience Replay
2. Multi-step Returns (n-step TD)
3. Double DQN 적용
4. Noisy Networks for exploration
5. Reward normalization/standardization
