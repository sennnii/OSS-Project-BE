# QMIX 성능 개선 완료 보고서

## 📊 문제점 요약

기존 실행 결과에서 발견된 주요 문제:

```
누적 수익: -0.0071
샤프 비율: -1.494 (음수)
승률: 45.37%
학습 보상: -0.0277 (계속 음수)
Q-values: -3.9 ~ -17.4 (모두 강한 음수)
```

## ✅ 적용된 개선 사항

### 1. 보상 스케일 대폭 증가 ⭐
- `REWARD_SCALE: 0.01 → 100.0` (10000배 증가)
- **효과**: Q-value collapse 방지, 강력한 학습 시그널

### 2. 보상 함수 최적화
- 실현 수익 2배 강화
- 거래 비용 50% 감소
- 다양성 보너스 10배 증가
- 보상 클리핑 제거 (학습 시그널 보존)

### 3. Warmup Phase 추가 🔥
- 초기 5,000 스텝 동안 랜덤 탐험
- 다양한 경험 수집 후 학습 시작
- 안정적인 학습 초기화

### 4. 탐험 전략 강화
- 최소 epsilon: 0.01 → 0.05
- 감소 기간: 100,000 → 150,000 스텝
- 충분한 탐험 시간 확보

### 5. 적극적 학습
- 초기 50 에피소드: 스텝당 8번 학습
- 이후: 스텝당 4번 학습
- 빠른 초기 학습 + 안정적 후기 학습

### 6. 모니터링 강화 📈
실시간 표시 항목:
- **R**: 에피소드 보상
- **Avg**: 10 에피소드 이동평균
- **Best**: 최고 보상
- **Q**: 평균 Q-value (양수 전환 확인)
- **L**: 학습 Loss (감소 추세 확인)

### 7. 학습 시간 확대
- 에피소드: 100 → 200

## 🎯 예상 개선 효과

### 학습 성능
| 지표 | 기존 | 예상 |
|------|------|------|
| 평균 보상 | -0.0277 | **+5 ~ +20** |
| Q-value | -3.9 ~ -17.4 | **+3 ~ +10** |
| 수렴 속도 | 느림 | **빠름** |

### 백테스트 성능
| 지표 | 기존 | 목표 |
|------|------|------|
| 누적 수익 | -0.0071 | **양수** |
| 샤프 비율 | -1.494 | **> 0.5** |
| 승률 | 45.37% | **> 55%** |
| MDD | -0.0107 | **감소** |

### 행동 분포
- Hold 편향 감소
- Buy/Sell 균형 개선
- 다양한 전략 학습

## 🔍 모니터링 가이드

### 출력 예시
```
Ep 10/200 | Eps: 0.950 | R: 15.32 | Avg: 12.45 | Best: 18.20 | Q: 5.67 | L: 0.0234 | Time: 8.5m
```

### 확인 포인트
1. **R (보상)**: 에피소드가 진행되면서 증가해야 함
2. **Q (Q-value)**: 음수 → 양수로 전환되어야 함
3. **L (Loss)**: 초기에 높다가 점차 감소해야 함
4. **Avg**: 우상향 추세를 보여야 함

### 학습이 잘 되고 있는지 확인
✅ **좋은 신호**:
- Q-value가 점점 증가 (특히 양수로 전환)
- Loss가 점점 감소
- Avg가 우상향
- Best 기록이 자주 갱신

⚠️ **나쁜 신호**:
- Q-value가 계속 음수
- Loss가 증가하거나 발산
- Avg가 계속 음수
- Best가 오랫동안 갱신 안 됨

## 📁 변경된 파일

1. **config.py** - 하이퍼파라미터 최적화
2. **environment.py** - 보상 함수 개선
3. **main.py** - Warmup + 모니터링 추가
4. **qmix_model.py** - 메트릭 추적

## 🚀 실행 방법

```bash
python main.py
```

기존과 동일하게 실행하면 모든 개선사항이 자동 적용됩니다.

## ⏱️ 예상 실행 시간

- **GPU (CUDA)**: 약 80-100분
- **CPU**: 약 150-200분

Warmup + 200 에피소드로 인해 기존보다 약간 더 오래 걸립니다.

## 💡 주의사항

1. **첫 실행**: Warmup 완료 메시지 확인
   ```
   Warmup complete! Starting policy learning...
   ```

2. **Q-value 모니터링**: 처음엔 음수지만 점차 양수로 전환되는지 확인

3. **메모리**: 버퍼 크기가 100,000으로 증가했으므로 충분한 메모리 필요

4. **GPU 권장**: 학습 속도가 크게 향상됨

## 🎓 핵심 개선 원리

### Q-value Collapse 해결
**문제**: 보상이 너무 작아서 Q-value가 계속 음수
**해결**: REWARD_SCALE을 100.0으로 증가 → 강한 학습 시그널

### 탐험 부족 해결
**문제**: 빠른 epsilon 감소로 로컬 최적해에 갇힘
**해결**: Warmup + 느린 epsilon 감소 → 충분한 탐험

### 학습 불안정 해결
**문제**: 초기 랜덤 경험 부족으로 불안정한 학습
**해결**: 5,000 스텝 warmup → 안정적인 학습 시작

## 📈 기대 결과

실행 후 다음과 같은 결과를 기대할 수 있습니다:

1. **학습 중**:
   - 에피소드 50: Q ≈ 0 ~ +2, R ≈ +5 ~ +10
   - 에피소드 100: Q ≈ +3 ~ +5, R ≈ +10 ~ +15
   - 에피소드 200: Q ≈ +5 ~ +8, R ≈ +15 ~ +25

2. **백테스트**:
   - 누적 수익: 양수
   - 샤프 비율: 0.5 이상
   - 승률: 55% 이상

3. **최종 신호**:
   - 더 합리적인 매수/매도 판단
   - Q-value가 양수로 의미 있는 신뢰도

## 🔧 문제 해결

### Q-value가 계속 음수인 경우
- REWARD_SCALE을 더 증가 (예: 200.0)
- NUM_EPISODES를 증가 (예: 300)

### Loss가 발산하는 경우
- Learning Rate 감소 (LR: 3e-4 → 1e-4)
- 학습 빈도 감소 (8,4 → 4,2)

### 메모리 부족
- BUFFER_SIZE 감소 (1e5 → 5e4)
- BATCH_SIZE 감소 (128 → 64)

## 📚 참고 자료

- IMPROVEMENTS.md: 상세 기술 문서 (한글)
- IMPROVEMENTS_EN.md: 상세 기술 문서 (영어)

---

**문의사항이 있으시면 이슈를 등록해주세요!**
