import argparse
import torch
import numpy as np
import pandas as pd
import time 

from config import (
    DEVICE, N_AGENTS, WINDOW_SIZE, BUFFER_SIZE, BATCH_SIZE, 
    TARGET_UPDATE_FREQ, NUM_EPISODES, EPSILON_START, EPSILON_END, EPSILON_DECAY_STEPS, WARMUP_STEPS,
    TRAIN_FREQUENCY, UPDATES_PER_STEP_EARLY, UPDATES_PER_STEP_LATE, EARLY_EPISODE_THRESHOLD,
    EARLY_STOPPING_PATIENCE, EARLY_STOPPING_MIN_DELTA
)
from data_processor import DataProcessor
from environment import MARLStockEnv
from qmix_model import QMIX_Learner
from replay_buffer import ReplayBuffer

def convert_joint_action_to_signal(joint_action, action_map):
    action_to_score = {"Long": 1, "Hold": 0, "Short": -1}
    score = sum(action_to_score[action_map[a]] for a in joint_action)
    
    if score >= 3:
        return "ì ê·¹ ë§¤ìˆ˜"
    elif score > 0:
        return "ë§¤ìˆ˜"
    elif score == 0:
        return "ë³´ìœ "
    elif score < 0 and score > -3:
        return "ë§¤ë„"
    elif score <= -3:
        return "ì ê·¹ ë§¤ë„"
    return "ë³´ìœ "

def generate_ai_explanation(final_signal, agent_analyses):
    all_importances = {}
    for _, _, importance_list in agent_analyses:
        for feature, imp in importance_list:
            all_importances[feature] = all_importances.get(feature, 0.0) + imp
            
    sorted_features = sorted(all_importances.items(), key=lambda item: item[1], reverse=True)
    
    explanation = f"AIê°€ '{final_signal}'ì„ ê²°ì •í•œ ì£¼ëœ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n"
    
    if not sorted_features:
        return explanation + "ë°ì´í„° ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤."
        
    top_feature_1 = sorted_features[0][0]
    explanation += f"  1. '{top_feature_1}' ì§€í‘œì˜ ìµœê·¼ ì›€ì§ì„ì„ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ê³ ë ¤í–ˆìŠµë‹ˆë‹¤.\n"
    
    if len(sorted_features) > 1:
        top_feature_2 = sorted_features[1][0]
        explanation += f"  2. '{top_feature_2}' ì§€í‘œê°€ 2ìˆœìœ„ë¡œ ê²°ì •ì— ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤.\n"
        
    if len(sorted_features) > 2:
        top_feature_3 = sorted_features[2][0]
        explanation += f"  3. ë§ˆì§€ë§‰ìœ¼ë¡œ '{top_feature_3}' ì§€í‘œë¥¼ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤.\n"
        
    return explanation

def print_ui_output(
    final_signal, 
    ai_explanation, 
    current_indicators, 
    q_total_grid,
    best_q_total_value, 
    action_names
):
    print("\n\n=============================================")
    print("      [ ğŸ“± ë¦¬ë¸Œë¦¬ AI ë¶„ì„ ê²°ê³¼ (ì‚¼ì„±ì „ì) ]")
    print("=============================================")
    
    print("\n--- 1. AI ìµœì¢… ì‹ í˜¸ ---")
    print(f"    {final_signal}")
    print(f"    (ì˜ˆìƒ íŒ€ Q-Value: {best_q_total_value:.4f})")
    
    print("\n--- 2. AI ì„¤ëª… ---")
    print(ai_explanation)
    
    print("\n--- 3. ê¸°ìˆ ì  ë¶„ì„ ìƒì„¸ (ìµœì¢…ì¼ ê¸°ì¤€) ---")
    print("    (AIê°€ ì…ìˆ˜í•˜ì—¬ ë¶„ì„í•œ ì›ë³¸ ë°ì´í„°ì…ë‹ˆë‹¤.)\n")
    technical_indicators = [
        'SMA20', 'MACD', 'MACD_Signal', 'RSI', 'Stoch_K', 'Stoch_D', 
        'ATR', 'Bollinger_B', 'VIX'
    ]
    fundamental_indicators = ['ROA', 'DebtRatio', 'AnalystRating']
    
    for indicator in technical_indicators:
        if indicator in current_indicators.index:
            print(f"    - {indicator:<13}: {current_indicators[indicator]:.2f}")
            
    print("\n    (í€ë”ë©˜íƒˆ ë° ê¸°íƒ€ ë°ì´í„°)\n")
    for indicator in fundamental_indicators:
         if indicator in current_indicators.index:
            print(f"    - {indicator:<13}: {current_indicators[indicator]:.2f}")
            
    print("\n--- 4. (ì°¸ê³ ) ìƒì„¸ Q_total ê·¸ë¦¬ë“œ ---")
    print("    (ëª¨ë“  í–‰ë™ ì¡°í•©ì˜ Q_total ê°’ì…ë‹ˆë‹¤.)\n")
    
    for k, a2_name in enumerate(action_names):
        print(f"    --- [Agent 2 (ì‹œì¥/í€ë”ë©˜íƒˆ) = {a2_name}] ---")
        col_names = " (A0)       | " + " | ".join([f"{name.center(10)}" for name in action_names]) + " (A1)"
        print("    " + col_names)
        print("    " + "-" * (11 + (13 * len(action_names))))
        
        for i, a0_name in enumerate(action_names):
            row_str = f" {a0_name:<9} | "
            for j in range(len(action_names)):
                row_str += f"{q_total_grid[i, j, k]:>10.4f} | "
            print("    " + row_str)
        print("") 
        
    print("=============================================")


def main():
    start_time = time.time()
    
    parser = argparse.ArgumentParser(description="QMIX Stock Trading AI")
    parser.add_argument('--quantity', type=int, default=0, help="í˜„ì¬ ë³´ìœ  ì£¼ì‹ ìˆ˜ëŸ‰")
    parser.add_argument('--price', type=float, default=0.0, help="í‰ë‹¨ê°€")
    args = parser.parse_args()
    
    pos_signal = 0
    entry_price = 0.0
    if args.quantity > 0: pos_signal = 1
    elif args.quantity < 0: 
        print("ê²½ê³ : ë§ˆì´ë„ˆìŠ¤ ìˆ˜ëŸ‰ â†’ ìˆ í¬ì§€ì…˜")
        pos_signal = -1
    if pos_signal != 0: entry_price = args.price
            
    user_portfolio = {
        'positions': [pos_signal] * N_AGENTS,
        'entry_prices': [entry_price] * N_AGENTS
    }

    print(f"ì‚¬ìš© ì¥ì¹˜: {DEVICE}")

    processor = DataProcessor()
    
    (features_unnormalized_df, prices_df, feature_names,
     agent_0_cols, agent_1_cols, agent_2_cols) = processor.process() 

    split_idx = int(len(features_unnormalized_df) * 0.8)
    if split_idx < WINDOW_SIZE * 2:
        print("ì˜¤ë¥˜: ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")
        return

    train_features_unnorm = features_unnormalized_df.iloc[:split_idx]
    train_prices = prices_df.iloc[:split_idx]
    test_features_unnorm = features_unnormalized_df.iloc[split_idx:]
    test_prices = prices_df.iloc[split_idx:]

    train_features, test_features = processor.normalize_data(
        train_features_unnorm, 
        test_features_unnorm
    )

    train_env = MARLStockEnv(
        train_features, train_prices, 
        agent_0_cols, agent_1_cols, agent_2_cols,
        n_agents=N_AGENTS, window_size=WINDOW_SIZE
    )
    test_env = MARLStockEnv(
        test_features, test_prices, 
        agent_0_cols, agent_1_cols, agent_2_cols,
        n_agents=N_AGENTS, window_size=WINDOW_SIZE
    )
    
    obs_dim_0 = train_env.observation_dim_0
    obs_dim_1 = train_env.observation_dim_1
    obs_dim_2 = train_env.observation_dim_2 
    obs_dims_list = [obs_dim_0, obs_dim_1, obs_dim_2]
    
    state_dim = train_env.state_dim
    action_dim = train_env.action_dim
    n_features = train_env.n_features_global

    learner = QMIX_Learner(obs_dims_list, action_dim, state_dim, DEVICE)
    buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, DEVICE)

    total_steps = 0
    warmup_done = False
    
    # [ê°œì„ ] í•™ìŠµ í†µê³„ ì¶”ì 
    episode_rewards = []
    episode_losses = []
    episode_q_values = []
    best_reward = -np.inf
    
    # [ì„±ëŠ¥ ìµœì í™”] ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ ë³€ìˆ˜
    no_improvement_count = 0
    best_avg_reward = -np.inf
    
    print(f"\n--- QMIX {NUM_EPISODES} ì—í”¼ì†Œë“œ í•™ìŠµ ì‹œì‘ ---")
    print(f"--- Obs: A0={obs_dim_0}, A1={obs_dim_1}, A2={obs_dim_2} | State={state_dim} ---")
    print(f"--- Warmup: {WARMUP_STEPS} steps with random actions ---")
    print(f"--- ì¡°ê¸° ì¢…ë£Œ: patience={EARLY_STOPPING_PATIENCE}, min_delta={EARLY_STOPPING_MIN_DELTA} ---")
    
    for i_episode in range(NUM_EPISODES):
        obs_dict, info = train_env.reset(initial_portfolio=None) 
        global_state = info["global_state"]
        episode_team_reward = 0.0
        episode_loss = 0.0
        episode_q_val = 0.0
        train_count = 0
        
        done = False
        
        while not done:
            total_steps += 1
            
            # [ê°œì„ ] Warmup phase - random exploration
            if total_steps <= WARMUP_STEPS:
                epsilon = 1.0
                if total_steps == WARMUP_STEPS:
                    print(f"Warmup complete! Starting policy learning...")
                    warmup_done = True
            else:
                # [ê°œì„ ] ì„ í˜• ê°ì†Œ Epsilon
                epsilon = max(
                    EPSILON_END, 
                    EPSILON_START - (EPSILON_START - EPSILON_END) * (total_steps - WARMUP_STEPS) / EPSILON_DECAY_STEPS
                )
            
            actions_dict = learner.select_actions(obs_dict, epsilon)
            next_obs_dict, rewards_dict, dones_dict, _, info = train_env.step(actions_dict)
            
            next_global_state = info["global_state"]
            team_reward = rewards_dict['agent_0']
            done = dones_dict['__all__']
            
            buffer.add(global_state, obs_dict, actions_dict, team_reward, 
                       next_global_state, next_obs_dict, done)
                       
            # [ì„±ëŠ¥ ìµœì í™”] í•™ìŠµì€ warmup í›„ì—ë§Œ, ë¹ˆë„ë¥¼ ë‚®ì¶¤
            if warmup_done and len(buffer) >= BATCH_SIZE * 2 and total_steps % TRAIN_FREQUENCY == 0:
                # ì´ˆë°˜ì—ëŠ” ë” ë§ì´ í•™ìŠµí•˜ë˜, ê³¼ë„í•˜ì§€ ì•Šê²Œ
                num_updates = UPDATES_PER_STEP_EARLY if i_episode < EARLY_EPISODE_THRESHOLD else UPDATES_PER_STEP_LATE
                for _ in range(num_updates):
                    loss, q_val = learner.train(buffer)
                    if loss is not None:
                        episode_loss += loss
                        episode_q_val += q_val
                        train_count += 1
            
            episode_team_reward += team_reward
            obs_dict = next_obs_dict
            global_state = next_global_state

            if warmup_done and total_steps % TARGET_UPDATE_FREQ == 0:
                learner.update_target_networks()
        
        episode_rewards.append(episode_team_reward)
        if train_count > 0:
            episode_losses.append(episode_loss / train_count)
            episode_q_values.append(episode_q_val / train_count)
        
        # [ê°œì„ ] Best ëª¨ë¸ ì €ì¥
        if episode_team_reward > best_reward:
            best_reward = episode_team_reward
            # torch.save(learner.state_dict(), 'best_model.pth')

        # [ì„±ëŠ¥ ìµœì í™”] ì¡°ê¸° ì¢…ë£Œ ì²´í¬
        if len(episode_rewards) >= 10:
            current_avg_reward = np.mean(episode_rewards[-10:])
            if current_avg_reward > best_avg_reward + EARLY_STOPPING_MIN_DELTA:
                best_avg_reward = current_avg_reward
                no_improvement_count = 0
            else:
                no_improvement_count += 1
            
            # ì¶©ë¶„í•œ ì—í”¼ì†Œë“œ í›„ ì¡°ê¸° ì¢…ë£Œ
            if i_episode >= 50 and no_improvement_count >= EARLY_STOPPING_PATIENCE:
                print(f"\nì¡°ê¸° ì¢…ë£Œ: {no_improvement_count} ì—í”¼ì†Œë“œ ë™ì•ˆ ê°œì„  ì—†ìŒ")
                print(f"ìµœê³  í‰ê·  ë³´ìƒ: {best_avg_reward:.2f}")
                break

        # [ìˆ˜ì •] ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤ ì¶œë ¥ + ì‹œê°„ í‘œì‹œ
        ep_time = time.time() - start_time
        
        if (i_episode + 1) <= 10 or (i_episode + 1) % 10 == 0:
            if len(episode_rewards) >= 10:
                avg_reward = np.mean(episode_rewards[-10:])
            else:
                avg_reward = np.mean(episode_rewards)
            
            # Q-valueì™€ Loss ì¶œë ¥ ì¶”ê°€
            if len(episode_q_values) > 0:
                avg_q = np.mean(episode_q_values[-10:]) if len(episode_q_values) >= 10 else np.mean(episode_q_values)
                avg_loss = np.mean(episode_losses[-10:]) if len(episode_losses) >= 10 else np.mean(episode_losses)
                print(f"Ep {i_episode+1}/{NUM_EPISODES} | "
                      f"Eps: {epsilon:.3f} | "
                      f"R: {episode_team_reward:.2f} | "
                      f"Avg: {avg_reward:.2f} | "
                      f"Best: {best_reward:.2f} | "
                      f"Q: {avg_q:.2f} | "
                      f"L: {avg_loss:.4f} | "
                      f"Time: {ep_time/60:.1f}m")
            else:
                print(f"Ep {i_episode+1}/{NUM_EPISODES} | "
                      f"Eps: {epsilon:.3f} | "
                      f"R: {episode_team_reward:.2f} | "
                      f"Avg: {avg_reward:.2f} | "
                      f"Best: {best_reward:.2f} | "
                      f"Time: {ep_time/60:.1f}m")

    print("--- í•™ìŠµ ì™„ë£Œ ---")

    # [ê°œì„ ] í•™ìŠµ ê³¡ì„  ë¶„ì„
    print("\n--- í•™ìŠµ ê³¡ì„  ë¶„ì„ ---")
    if len(episode_rewards) >= 100:
        print(f"    - ì´ˆê¸° 100 ì—í”¼ì†Œë“œ í‰ê· : {np.mean(episode_rewards[:100]):.2f}")
        print(f"    - ìµœì¢… 100 ì—í”¼ì†Œë“œ í‰ê· : {np.mean(episode_rewards[-100:]):.2f}")
    else:
        print(f"    - ì´ˆê¸° 50 ì—í”¼ì†Œë“œ í‰ê· : {np.mean(episode_rewards[:min(50, len(episode_rewards))]):.2f}")
        print(f"    - ìµœì¢… 50 ì—í”¼ì†Œë“œ í‰ê· : {np.mean(episode_rewards[-min(50, len(episode_rewards)):]):.2f}")
    print(f"    - ìµœê³  ì—í”¼ì†Œë“œ ë³´ìƒ: {best_reward:.2f}")

    print("\n--- [1] ì „ì²´ í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë°±í…ŒìŠ¤íŠ¸ ---")
    
    obs_dict, info = test_env.reset(initial_portfolio=user_portfolio)
    global_state = info["global_state"]
    
    all_team_rewards = []
    all_actions_log = []
    
    current_step = 0
    while current_step < test_env.max_steps:
        actions_dict = learner.select_actions(obs_dict, 0.0)
        all_actions_log.append(list(actions_dict.values()))
        
        obs_dict, rewards_dict, dones_dict, _, info = test_env.step(actions_dict)
        
        all_team_rewards.append(rewards_dict['agent_0'])
        
        global_state = info["global_state"]
        current_step += 1
        if dones_dict['__all__']:
            break

    print("\n--- [2] ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì§€í‘œ ---")
    test_days = len(all_team_rewards)
    if test_days > 0:
        reward_series = pd.Series(all_team_rewards)
        
        total_return = reward_series.sum()
        daily_std = reward_series.std() + 1e-9
        sharpe_ratio = (reward_series.mean() / daily_std) * np.sqrt(252)
        win_days = (reward_series > 0).sum()
        win_rate = (win_days / test_days) * 100.0
        
        # [ê°œì„ ] ì¶”ê°€ ì„±ëŠ¥ ì§€í‘œ
        max_drawdown = (reward_series.cumsum() - reward_series.cumsum().cummax()).min()
        
        print(f"    - ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„: {test_days} ì¼")
        print(f"    - ëˆ„ì  ìˆ˜ìµ: {total_return:.2f}")
        print(f"    - ì¼ í‰ê·  ìˆ˜ìµ: {reward_series.mean():.4f}")
        print(f"    - ì¼ ìˆ˜ìµ ë³€ë™ì„±: {daily_std:.4f}")
        print(f"    - ìƒ¤í”„ ë¹„ìœ¨ (ì—°í™˜ì‚°): {sharpe_ratio:.3f}")
        print(f"    - ìŠ¹ë¥ : {win_rate:.2f}% ({win_days}/{test_days} ì¼)")
        print(f"    - ìµœëŒ€ ë‚™í­(MDD): {max_drawdown:.2f}")
        
        # [ê°œì„ ] í–‰ë™ ë¶„í¬ ë¶„ì„
        actions_array = np.array(all_actions_log)
        print(f"\n    - í–‰ë™ ë¶„í¬:")
        for i in range(N_AGENTS):
            agent_actions = actions_array[:, i]
            buy_pct = (agent_actions == 0).sum() / len(agent_actions) * 100
            hold_pct = (agent_actions == 1).sum() / len(agent_actions) * 100
            sell_pct = (agent_actions == 2).sum() / len(agent_actions) * 100
            print(f"      Agent {i}: Buy={buy_pct:.1f}% Hold={hold_pct:.1f}% Sell={sell_pct:.1f}%")
    else:
        print("    - ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ì´ 0ì¼ì…ë‹ˆë‹¤.")

    # --- ìµœì¢…ì¼ ë¶„ì„ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€) ---
    print("\n--- [3] ìµœì¢…ì¼ ì˜ˆì¸¡ ìƒì„¸ ë¶„ì„ ---")
    
    final_obs_dict = obs_dict
    action_map = {0: "Long", 1: "Hold", 2: "Short"}
    action_indices = list(action_map.keys())
    action_names = list(action_map.values())
    
    obs_tensors = [torch.FloatTensor(final_obs_dict[f'agent_{i}']).unsqueeze(0).to(DEVICE) for i in range(N_AGENTS)]
    state_tensor = torch.FloatTensor(global_state).unsqueeze(0).to(DEVICE)
    
    q_vals_all_agents = []
    with torch.no_grad():
        for i, agent in enumerate(learner.agents):
            q_vals_all_agents.append(agent.get_q_values(obs_tensors[i]))

    agent_q_inputs = []
    action_tuples = []
    
    q_vals_0 = q_vals_all_agents[0].squeeze(0)
    q_vals_1 = q_vals_all_agents[1].squeeze(0)
    q_vals_2 = q_vals_all_agents[2].squeeze(0)

    for i, a0_idx in enumerate(action_indices):
        for j, a1_idx in enumerate(action_indices):
            for k, a2_idx in enumerate(action_indices): 
                q0 = q_vals_0[a0_idx]
                q1 = q_vals_1[a1_idx]
                q2 = q_vals_2[a2_idx]
                agent_q_inputs.append(torch.stack([q0, q1, q2])) 
                action_tuples.append((a0_idx, a1_idx, a2_idx))
    
    agent_q_batch = torch.stack(agent_q_inputs) 
    state_batch = state_tensor.repeat(len(action_tuples), 1)

    with torch.no_grad():
        all_q_totals = learner.mixer(agent_q_batch, state_batch)
    
    q_total_grid = all_q_totals.view(
        len(action_indices), len(action_indices), len(action_indices) 
    ).cpu().numpy()
    
    best_q_total_value = all_q_totals.max().item()
    best_joint_action_idx_flat = all_q_totals.argmax().item()
    best_joint_action_indices = action_tuples[best_joint_action_idx_flat]
    
    agent_analyses = []
    feature_names_list = [agent_0_cols, agent_1_cols, agent_2_cols] 
    n_features_list = [
        train_env.n_features_agent_0, 
        train_env.n_features_agent_1, 
        train_env.n_features_agent_2
    ]
    
    for i, agent in enumerate(learner.agents):
        obs = final_obs_dict[f'agent_{i}']
        agent_feature_names = feature_names_list[i]
        n_features_agent = n_features_list[i]

        action_idx, q_values, importance = agent.get_prediction_with_reason(
            obs, 
            agent_feature_names,
            WINDOW_SIZE, 
            n_features_agent
        )
        agent_analyses.append((action_idx, q_values, importance))
        
    final_signal = convert_joint_action_to_signal(best_joint_action_indices, action_map)
    ai_explanation = generate_ai_explanation(final_signal, agent_analyses)
    
    current_indicator_values = test_features_unnorm.iloc[-1]
    
    print_ui_output(
        final_signal=final_signal,
        ai_explanation=ai_explanation,
        current_indicators=current_indicator_values,
        q_total_grid=q_total_grid,
        best_q_total_value=best_q_total_value,
        action_names=action_names
    )
    
    end_time = time.time()
    total_time = end_time - start_time
    print("\n=============================================")
    print(f"  [ ğŸ“Š ì´ ì‹¤í–‰ ì‹œê°„ ]")
    print(f"    {total_time // 60:.0f} ë¶„ {total_time % 60:.2f} ì´ˆ")
    print("=============================================")


if __name__ == "__main__":
    main()