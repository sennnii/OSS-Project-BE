import argparse
import torch
import numpy as np
import pandas as pd
import time 

from config import (
    DEVICE, N_AGENTS, WINDOW_SIZE, BUFFER_SIZE, BATCH_SIZE, 
    TARGET_UPDATE_FREQ, NUM_EPISODES, EPSILON_START, EPSILON_END, EPSILON_DECAY_STEPS, WARMUP_STEPS
)
from data_processor import DataProcessor
from environment import MARLStockEnv
from qmix_model import QMIX_Learner
from replay_buffer import ReplayBuffer

def convert_joint_action_to_signal(joint_action, action_map):
    action_to_score = {"Long": 1, "Hold": 0, "Short": -1}
    score = sum(action_to_score[action_map[a]] for a in joint_action)
    
    if score >= 3:
        return "ì ê·¹ ë§¤ìˆ˜"
    elif score > 0:
        return "ë§¤ìˆ˜"
    elif score == 0:
        return "ë³´ìœ "
    elif score < 0 and score > -3:
        return "ë§¤ë„"
    elif score <= -3:
        return "ì ê·¹ ë§¤ë„"
    return "ë³´ìœ "

def generate_ai_explanation(final_signal, agent_analyses):
    all_importances = {}
    for _, _, importance_list in agent_analyses:
        for feature, imp in importance_list:
            all_importances[feature] = all_importances.get(feature, 0.0) + imp
            
    sorted_features = sorted(all_importances.items(), key=lambda item: item[1], reverse=True)
    
    explanation = f"AIê°€ '{final_signal}'ì„ ê²°ì •í•œ ì£¼ëœ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n"
    
    if not sorted_features:
        return explanation + "ë°ì´í„° ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤."
        
    top_feature_1 = sorted_features[0][0]
    explanation += f"  1. '{top_feature_1}' ì§€í‘œì˜ ìµœê·¼ ì›€ì§ì„ì„ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ê³ ë ¤í–ˆìŠµë‹ˆë‹¤.\n"
    
    if len(sorted_features) > 1:
        top_feature_2 = sorted_features[1][0]
        explanation += f"  2. '{top_feature_2}' ì§€í‘œê°€ 2ìˆœìœ„ë¡œ ê²°ì •ì— ì˜í–¥ì„ ë¯¸ì³¤ìŠµë‹ˆë‹¤.\n"
        
    if len(sorted_features) > 2:
        top_feature_3 = sorted_features[2][0]
        explanation += f"  3. ë§ˆì§€ë§‰ìœ¼ë¡œ '{top_feature_3}' ì§€í‘œë¥¼ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤.\n"
        
    return explanation

def print_ui_output(
    final_signal, 
    ai_explanation, 
    current_indicators, 
    q_total_grid,
    best_q_total_value, 
    action_names
):
    print("\n\n=============================================")
    print("      [ ğŸ“± ë¦¬ë¸Œë¦¬ AI ë¶„ì„ ê²°ê³¼ (ì‚¼ì„±ì „ì) ]")
    print("=============================================")
    
    print("\n--- 1. AI ìµœì¢… ì‹ í˜¸ ---")
    print(f"    {final_signal}")
    print(f"    (ì˜ˆìƒ íŒ€ Q-Value: {best_q_total_value:.4f})")
    
    print("\n--- 2. AI ì„¤ëª… ---")
    print(ai_explanation)
    
    print("\n--- 3. ê¸°ìˆ ì  ë¶„ì„ ìƒì„¸ (ìµœì¢…ì¼ ê¸°ì¤€) ---")
    print("    (AIê°€ ì…ìˆ˜í•˜ì—¬ ë¶„ì„í•œ ì›ë³¸ ë°ì´í„°ì…ë‹ˆë‹¤.)\n")
    technical_indicators = [
        'SMA20', 'MACD', 'MACD_Signal', 'RSI', 'Stoch_K', 'Stoch_D', 
        'ATR', 'Bollinger_B', 'VIX'
    ]
    fundamental_indicators = ['ROA', 'DebtRatio', 'AnalystRating']
    
    for indicator in technical_indicators:
        if indicator in current_indicators.index:
            print(f"    - {indicator:<13}: {current_indicators[indicator]:.2f}")
            
    print("\n    (í€ë”ë©˜íƒˆ ë° ê¸°íƒ€ ë°ì´í„°)\n")
    for indicator in fundamental_indicators:
         if indicator in current_indicators.index:
            print(f"    - {indicator:<13}: {current_indicators[indicator]:.2f}")
            
    print("\n--- 4. (ì°¸ê³ ) ìƒì„¸ Q_total ê·¸ë¦¬ë“œ ---")
    print("    (ëª¨ë“  í–‰ë™ ì¡°í•©ì˜ Q_total ê°’ì…ë‹ˆë‹¤.)\n")
    
    for k, a2_name in enumerate(action_names):
        print(f"    --- [Agent 2 (ì‹œì¥/í€ë”ë©˜íƒˆ) = {a2_name}] ---")
        col_names = " (A0)       | " + " | ".join([f"{name.center(10)}" for name in action_names]) + " (A1)"
        print("    " + col_names)
        print("    " + "-" * (11 + (13 * len(action_names))))
        
        for i, a0_name in enumerate(action_names):
            row_str = f" {a0_name:<9} | "
            for j in range(len(action_names)):
                row_str += f"{q_total_grid[i, j, k]:>10.4f} | "
            print("    " + row_str)
        print("") 
        
    print("=============================================")


def test_model(learner, test_env, episodes=5):
    """ëª¨ë¸ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜"""
    total_rewards = []
    
    for _ in range(episodes):
        obs_dict, info = test_env.reset(initial_portfolio=None)
        episode_reward = 0.0
        done = False
        
        while not done:
            actions_dict = learner.select_actions(obs_dict, epsilon=0.0)  # greedy
            obs_dict, rewards_dict, dones_dict, _, info = test_env.step(actions_dict)
            episode_reward += rewards_dict['agent_0']
            done = dones_dict['__all__']
        
        total_rewards.append(episode_reward)
    
    return np.mean(total_rewards)


def main():
    start_time = time.time()
    
    parser = argparse.ArgumentParser(description="QMIX Stock Trading AI")
    parser.add_argument('--quantity', type=int, default=0, help="í˜„ì¬ ë³´ìœ  ì£¼ì‹ ìˆ˜ëŸ‰")
    parser.add_argument('--price', type=float, default=0.0, help="í‰ë‹¨ê°€")
    args = parser.parse_args()
    
    pos_signal = 0
    entry_price = 0.0
    if args.quantity > 0: pos_signal = 1
    elif args.quantity < 0: 
        print("ê²½ê³ : ë§ˆì´ë„ˆìŠ¤ ìˆ˜ëŸ‰ â†’ ìˆ í¬ì§€ì…˜")
        pos_signal = -1
    if pos_signal != 0: entry_price = args.price
            
    user_portfolio = {
        'positions': [pos_signal] * N_AGENTS,
        'entry_prices': [entry_price] * N_AGENTS
    }

    print(f"ì‚¬ìš© ì¥ì¹˜: {DEVICE}")

    processor = DataProcessor()
    
    (features_unnormalized_df, prices_df, feature_names,
     agent_0_cols, agent_1_cols, agent_2_cols) = processor.process() 

    split_idx = int(len(features_unnormalized_df) * 0.9)
    if split_idx < WINDOW_SIZE * 2:
        print("ì˜¤ë¥˜: ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")
        return

    train_features_unnorm = features_unnormalized_df.iloc[:split_idx]
    train_prices = prices_df.iloc[:split_idx]
    test_features_unnorm = features_unnormalized_df.iloc[split_idx:]
    test_prices = prices_df.iloc[split_idx:]

    train_features, test_features = processor.normalize_data(
        train_features_unnorm, 
        test_features_unnorm
    )

    train_env = MARLStockEnv(
        train_features, train_prices, 
        agent_0_cols, agent_1_cols, agent_2_cols,
        n_agents=N_AGENTS, window_size=WINDOW_SIZE
    )
    test_env = MARLStockEnv(
        test_features, test_prices, 
        agent_0_cols, agent_1_cols, agent_2_cols,
        n_agents=N_AGENTS, window_size=WINDOW_SIZE
    )
    
    obs_dim_0 = train_env.observation_dim_0
    obs_dim_1 = train_env.observation_dim_1
    obs_dim_2 = train_env.observation_dim_2 
    obs_dims_list = [obs_dim_0, obs_dim_1, obs_dim_2]
    
    state_dim = train_env.state_dim
    action_dim = train_env.action_dim
    n_features = train_env.n_features_global

    learner = QMIX_Learner(obs_dims_list, action_dim, state_dim, DEVICE)
    buffer = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, DEVICE)

    total_steps = 0
    warmup_done = False
    
    # [ê°œì„ ] í•™ìŠµ í†µê³„ ì¶”ì 
    episode_rewards = []
    episode_losses = []
    episode_q_values = []
    best_reward = -np.inf
    
    # ğŸ†• ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ ë³€ìˆ˜ ì¶”ê°€
    best_test_reward = -np.inf
    no_improve_count = 0
    patience = 5  # 100 ì—í”¼ì†Œë“œ(20*5) ë™ì•ˆ ê°œì„  ì—†ìœ¼ë©´ ì¤‘ë‹¨
    validation_interval = 20  # 20 ì—í”¼ì†Œë“œë§ˆë‹¤ ê²€ì¦
    early_stop = False
    
    print(f"\n--- QMIX {NUM_EPISODES} ì—í”¼ì†Œë“œ í•™ìŠµ ì‹œì‘ ---")
    print(f"--- Obs: A0={obs_dim_0}, A1={obs_dim_1}, A2={obs_dim_2} | State={state_dim} ---")
    print(f"--- Warmup: {WARMUP_STEPS} steps with random actions ---")
    print(f"--- ì¡°ê¸° ì¢…ë£Œ: {validation_interval} ì—í”¼ì†Œë“œë§ˆë‹¤ ê²€ì¦, patience={patience} ---")
    
    for i_episode in range(NUM_EPISODES):
        obs_dict, info = train_env.reset(initial_portfolio=None) 
        global_state = info["global_state"]
        episode_team_reward = 0.0
        episode_loss = 0.0
        episode_q_val = 0.0
        train_count = 0
        
        done = False
        
        while not done:
            total_steps += 1
            
            # [ê°œì„ ] Warmup phase - random exploration
            if total_steps <= WARMUP_STEPS:
                epsilon = 1.0
                if total_steps == WARMUP_STEPS:
                    print(f"Warmup complete! Starting policy learning...")
                    warmup_done = True
            else:
                # [ê°œì„ ] ì„ í˜• ê°ì†Œ Epsilon
                epsilon = max(
                    EPSILON_END, 
                    EPSILON_START - (EPSILON_START - EPSILON_END) * (total_steps - WARMUP_STEPS) / EPSILON_DECAY_STEPS
                )
            
            actions_dict = learner.select_actions(obs_dict, epsilon)
            next_obs_dict, rewards_dict, dones_dict, _, info = train_env.step(actions_dict)
            
            next_global_state = info["global_state"]
            team_reward = rewards_dict['agent_0']
            done = dones_dict['__all__']
            
            buffer.add(global_state, obs_dict, actions_dict, team_reward, 
                       next_global_state, next_obs_dict, done)
                       
            if warmup_done and len(buffer) >= BATCH_SIZE * 2:
                num_updates = 2 
                
                for _ in range(num_updates):
                    loss, q_val = learner.train(buffer)
                    if loss is not None:
                        episode_loss += loss
                        episode_q_val += q_val
                        train_count += 1
            
            episode_team_reward += team_reward
            obs_dict = next_obs_dict
            global_state = next_global_state

            if warmup_done and total_steps % TARGET_UPDATE_FREQ == 0:
                learner.update_target_networks()
        
        episode_rewards.append(episode_team_reward)
        if train_count > 0:
            episode_losses.append(episode_loss / train_count)
            episode_q_values.append(episode_q_val / train_count)
        
        # [ê°œì„ ] Best ëª¨ë¸ ì €ì¥
        if episode_team_reward > best_reward:
            best_reward = episode_team_reward

        # ğŸ†• ì¡°ê¸° ì¢…ë£Œ ë¡œì§ ì¶”ê°€
        if warmup_done and i_episode >= 50 and (i_episode + 1) % validation_interval == 0:
            print(f"\n[ê²€ì¦ ì¤‘... Ep {i_episode+1}]")
            test_reward = test_model(learner, test_env, episodes=3)
            
            if test_reward > best_test_reward:
                best_test_reward = test_reward
                torch.save(learner.state_dict(), 'best_model.pth')
                no_improve_count = 0
                print(f"âœ… ìƒˆë¡œìš´ ìµœê³  ê²€ì¦ ì„±ëŠ¥: {test_reward:.2f} (ëª¨ë¸ ì €ì¥ë¨)")
            else:
                no_improve_count += 1
                print(f"âš ï¸  ê²€ì¦ ì„±ëŠ¥: {test_reward:.2f} (ìµœê³ : {best_test_reward:.2f}, ì •ì²´: {no_improve_count}/{patience})")
            
            if no_improve_count >= patience:
                print(f"\nğŸ›‘ ì¡°ê¸° ì¢…ë£Œ: {i_episode + 1} ì—í”¼ì†Œë“œì—ì„œ í•™ìŠµ ì¤‘ë‹¨ (ì„±ëŠ¥ ì •ì²´)")
                early_stop = True

        # [ìˆ˜ì •] ë§¤ ì—í”¼ì†Œë“œë§ˆë‹¤ ì¶œë ¥ + ì‹œê°„ í‘œì‹œ
        ep_time = time.time() - start_time
        
        if (i_episode + 1) <= 10 or (i_episode + 1) % 10 == 0:
            if len(episode_rewards) >= 10:
                avg_reward = np.mean(episode_rewards[-10:])
            else:
                avg_reward = np.mean(episode_rewards)
            
            # Q-valueì™€ Loss ì¶œë ¥ ì¶”ê°€
            if len(episode_q_values) > 0:
                avg_q = np.mean(episode_q_values[-10:]) if len(episode_q_values) >= 10 else np.mean(episode_q_values)
                avg_loss = np.mean(episode_losses[-10:]) if len(episode_losses) >= 10 else np.mean(episode_losses)
                print(f"Ep {i_episode+1}/{NUM_EPISODES} | "
                      f"Eps: {epsilon:.3f} | "
                      f"R: {episode_team_reward:.2f} | "
                      f"Avg: {avg_reward:.2f} | "
                      f"Best: {best_reward:.2f} | "
                      f"Q: {avg_q:.2f} | "
                      f"L: {avg_loss:.4f} | "
                      f"Time: {ep_time/60:.1f}m")
            else:
                print(f"Ep {i_episode+1}/{NUM_EPISODES} | "
                      f"Eps: {epsilon:.3f} | "
                      f"R: {episode_team_reward:.2f} | "
                      f"Avg: {avg_reward:.2f} | "
                      f"Best: {best_reward:.2f} | "
                      f"Time: {ep_time/60:.1f}m")
        
        # ğŸ†• ì¡°ê¸° ì¢…ë£Œ ì²´í¬
        if early_stop:
            break

    # ğŸ†• ìµœê³  ëª¨ë¸ ë¡œë“œ (ì¡°ê¸° ì¢…ë£Œ ì‹œ)
    if early_stop and best_test_reward > -np.inf:
        print("\nìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        learner.load_state_dict(torch.load('best_model.pth'))

    print("--- í•™ìŠµ ì™„ë£Œ ---")

    # [ê°œì„ ] í•™ìŠµ ê³¡ì„  ë¶„ì„
    print("\n--- í•™ìŠµ ê³¡ì„  ë¶„ì„ ---")
    if len(episode_rewards) >= 100:
        print(f"    - ì´ˆê¸° 100 ì—í”¼ì†Œë“œ í‰ê· : {np.mean(episode_rewards[:100]):.2f}")
        print(f"    - ìµœì¢… 100 ì—í”¼ì†Œë“œ í‰ê· : {np.mean(episode_rewards[-100:]):.2f}")
    else:
        print(f"    - ì´ˆê¸° 50 ì—í”¼ì†Œë“œ í‰ê· : {np.mean(episode_rewards[:min(50, len(episode_rewards))]):.2f}")
        print(f"    - ìµœì¢… 50 ì—í”¼ì†Œë“œ í‰ê· : {np.mean(episode_rewards[-min(50, len(episode_rewards)):]):.2f}")
    print(f"    - ìµœê³  ì—í”¼ì†Œë“œ ë³´ìƒ: {best_reward:.2f}")
    print(f"    - ìµœê³  ê²€ì¦ ë³´ìƒ: {best_test_reward:.2f}")

    print("\n--- [1] ì „ì²´ í…ŒìŠ¤íŠ¸ ê¸°ê°„ ë°±í…ŒìŠ¤íŠ¸ ---")
    
    obs_dict, info = test_env.reset(initial_portfolio=user_portfolio)
    global_state = info["global_state"]
    
    all_team_rewards = []
    all_actions_log = []
    
    current_step = 0
    while current_step < test_env.max_steps:
        actions_dict = learner.select_actions(obs_dict, 0.0)
        all_actions_log.append(list(actions_dict.values()))
        
        obs_dict, rewards_dict, dones_dict, _, info = test_env.step(actions_dict)
        
        all_team_rewards.append(rewards_dict['agent_0'])
        
        global_state = info["global_state"]
        current_step += 1
        if dones_dict['__all__']:
            break

    print("\n--- [2] ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì§€í‘œ ---")
    test_days = len(all_team_rewards)
    if test_days > 0:
        reward_series = pd.Series(all_team_rewards)
        
        total_return = reward_series.sum()
        daily_std = reward_series.std() + 1e-9
        sharpe_ratio = (reward_series.mean() / daily_std) * np.sqrt(252)
        win_days = (reward_series > 0).sum()
        win_rate = (win_days / test_days) * 100.0
        
        # [ê°œì„ ] ì¶”ê°€ ì„±ëŠ¥ ì§€í‘œ
        max_drawdown = (reward_series.cumsum() - reward_series.cumsum().cummax()).min()
        
        print(f"    - ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„: {test_days} ì¼")
        print(f"    - ëˆ„ì  ìˆ˜ìµ: {total_return:.2f}")
        print(f"    - ì¼ í‰ê·  ìˆ˜ìµ: {reward_series.mean():.4f}")
        print(f"    - ì¼ ìˆ˜ìµ ë³€ë™ì„±: {daily_std:.4f}")
        print(f"    - ìƒ¤í”„ ë¹„ìœ¨ (ì—°í™˜ì‚°): {sharpe_ratio:.3f}")
        print(f"    - ìŠ¹ë¥ : {win_rate:.2f}% ({win_days}/{test_days} ì¼)")
        print(f"    - ìµœëŒ€ ë‚™í­(MDD): {max_drawdown:.2f}")
        
        # [ê°œì„ ] í–‰ë™ ë¶„í¬ ë¶„ì„
        actions_array = np.array(all_actions_log)
        print(f"\n    - í–‰ë™ ë¶„í¬:")
        for i in range(N_AGENTS):
            agent_actions = actions_array[:, i]
            buy_pct = (agent_actions == 0).sum() / len(agent_actions) * 100
            hold_pct = (agent_actions == 1).sum() / len(agent_actions) * 100
            sell_pct = (agent_actions == 2).sum() / len(agent_actions) * 100
            print(f"      Agent {i}: Buy={buy_pct:.1f}% Hold={hold_pct:.1f}% Sell={sell_pct:.1f}%")
    else:
        print("    - ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ì´ 0ì¼ì…ë‹ˆë‹¤.")
    
    # main.pyì˜ ê·¸ë˜í”„ ìƒì„± ë¶€ë¶„ (ë°±í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥ í›„) ìˆ˜ì •

    # ğŸ†• ë°±í…ŒìŠ¤íŠ¸ ê·¸ë˜í”„ ìƒì„± (KOSPI ë¹„êµ ì¶”ê°€)
    if test_days > 0:
        print("\n--- [3] Backtest Visualization ---")
        
        try:
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            import matplotlib.dates as mdates
            import matplotlib.font_manager as fm
            import yfinance as yf
            
            # í•œê¸€ í°íŠ¸ ì„¤ì •
            fm.fontManager = fm.FontManager()
            plt.rcParams['font.family'] = 'NanumGothic'
            plt.rcParams['axes.unicode_minus'] = False
            
            # ì´ˆê¸° ìë³¸
            initial_capital = 10_000_000
            
            # AI ì „ëµ ì‹œë®¬ë ˆì´ì…˜
            cash = initial_capital
            position = 0
            ai_values = [initial_capital]
            
            for step in range(test_days - 1):
                current_price = test_prices.iloc[step + WINDOW_SIZE]
                
                # AI í–‰ë™
                actions = all_actions_log[step]
                joint_action = sum([1 if a == 0 else (-1 if a == 2 else 0) for a in actions])
                
                # ë§¤ìˆ˜
                if joint_action >= 2 and position == 0:
                    shares = cash // current_price
                    position = shares
                    cash -= shares * current_price
                # ë§¤ë„
                elif joint_action <= -2 and position > 0:
                    cash += position * current_price
                    position = 0
                
                # í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜
                next_price = test_prices.iloc[step + WINDOW_SIZE + 1]
                portfolio_value = cash + (position * next_price)
                ai_values.append(portfolio_value)
            
            # Buy & Hold (ì‚¼ì„±ì „ì)
            initial_shares = initial_capital // test_prices.iloc[WINDOW_SIZE]
            buy_hold_values = []
            for step in range(test_days):
                price = test_prices.iloc[step + WINDOW_SIZE]
                buy_hold_values.append(initial_shares * price)
            
            # ğŸ†• KOSPI ì§€ìˆ˜ ë‹¤ìš´ë¡œë“œ ë° ê³„ì‚°
            kospi_values = []
            try:
                # í…ŒìŠ¤íŠ¸ ê¸°ê°„ì˜ ë‚ ì§œ ë²”ìœ„ ê°€ì ¸ì˜¤ê¸°
                test_start_date = test_prices.index[WINDOW_SIZE]
                test_end_date = test_prices.index[WINDOW_SIZE + test_days - 1]
                
                print(f"    KOSPI ì§€ìˆ˜ ë‹¤ìš´ë¡œë“œ ì¤‘ ({test_start_date.date()} ~ {test_end_date.date()})...")
                
                # KOSPI ì§€ìˆ˜ ë‹¤ìš´ë¡œë“œ (^KS11)
                kospi_data = yf.download('^KS11', 
                                        start=test_start_date, 
                                        end=test_end_date + pd.Timedelta(days=1),
                                        progress=False)
                
                if not kospi_data.empty and 'Close' in kospi_data.columns:
                    # ë‚ ì§œ ì¸ë±ìŠ¤ ì •ë¦¬
                    kospi_data.index = pd.to_datetime(kospi_data.index).tz_localize(None)
                    
                    # í…ŒìŠ¤íŠ¸ ê¸°ê°„ê³¼ ì¼ì¹˜í•˜ëŠ” KOSPI ë°ì´í„°ë§Œ ì¶”ì¶œ
                    kospi_aligned = kospi_data['Close'].reindex(
                        test_prices.index[WINDOW_SIZE:WINDOW_SIZE + test_days],
                        method='ffill'  # íœ´ì¥ì¼ì€ ì „ì¼ ê°’ìœ¼ë¡œ ì±„ì›€
                    )
                    
                    # KOSPI ì‹œì‘ ê°’ìœ¼ë¡œ ì •ê·œí™”
                    kospi_start = kospi_aligned.iloc[0]
                    for step in range(test_days):
                        kospi_price = kospi_aligned.iloc[step]
                        kospi_values.append(initial_capital * (kospi_price / kospi_start))
                    
                    print(f"    âœ… KOSPI ì§€ìˆ˜ ë¡œë“œ ì™„ë£Œ (ì‹œì‘: {kospi_start:.2f}, ì¢…ë£Œ: {kospi_aligned.iloc[-1]:.2f})")
                else:
                    raise Exception("KOSPI ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
                    
            except Exception as e:
                print(f"    âš ï¸  KOSPI ì§€ìˆ˜ ë¡œë“œ ì‹¤íŒ¨ ({e}), ì‚¼ì„±ì „ì ì£¼ê°€ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•©ë‹ˆë‹¤.")
                # KOSPI ì‹¤íŒ¨ì‹œ ì‚¼ì„±ì „ì ì£¼ê°€ë¡œ ëŒ€ì²´
                samsung_start = test_prices.iloc[WINDOW_SIZE]
                for step in range(test_days):
                    price = test_prices.iloc[step + WINDOW_SIZE]
                    kospi_values.append(initial_capital * (price / samsung_start))
            
            # ë‚ ì§œ ì¸ë±ìŠ¤ ìƒì„±
            if isinstance(test_prices.index, pd.DatetimeIndex):
                test_dates = test_prices.iloc[WINDOW_SIZE:WINDOW_SIZE + test_days].index
                use_dates = True
            else:
                test_dates = pd.date_range(start='2024-01-01', periods=test_days, freq='D')
                use_dates = True
            
            # ì„±ëŠ¥ ê³„ì‚°
            ai_final = ai_values[-1]
            bh_final = buy_hold_values[-1]
            kospi_final = kospi_values[-1] if kospi_values else initial_capital
            
            ai_return = (ai_final - initial_capital) / initial_capital * 100
            bh_return = (bh_final - initial_capital) / initial_capital * 100
            kospi_return = (kospi_final - initial_capital) / initial_capital * 100
            
            ai_returns = pd.Series(ai_values).pct_change().dropna()
            sharpe = (ai_returns.mean() / ai_returns.std()) * np.sqrt(252) if len(ai_returns) > 0 else 0
            
            downside_returns = ai_returns[ai_returns < 0]
            sortino = (ai_returns.mean() / downside_returns.std()) * np.sqrt(252) if len(downside_returns) > 0 else 0
            
            cumulative = pd.Series(ai_values)
            running_max = cumulative.cummax()
            drawdown = (cumulative - running_max) / running_max * 100
            mdd = drawdown.min()
            
            # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
            fig, ax = plt.subplots(figsize=(14, 8))
            
            # ì„  ê·¸ë¦¬ê¸° (ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ ìŠ¤íƒ€ì¼)
            ax.plot(test_dates, ai_values[:len(test_dates)], 
                   label=f'QMIX Agent (ìµœì¢…: {ai_final:,.0f} ì›)', 
                   linewidth=2, color='#1f77b4', linestyle='-')
            ax.plot(test_dates, buy_hold_values[:len(test_dates)], 
                   label=f'Buy & Hold (ìµœì¢…: {bh_final:,.0f} ì›)', 
                   linewidth=2, linestyle='--', color='#ff7f0e')
            ax.plot(test_dates, kospi_values[:len(test_dates)], 
                   label=f'KOSPI (ìµœì¢…: {kospi_final:,.0f} ì›)', 
                   linewidth=1.5, linestyle=':', color='#808080')
            
            # ì œëª©
            title_text = f'QMIX ë°±í…ŒìŠ¤íŠ¸ ì„±ê³¼ (ì´ˆê¸°ìê¸ˆ: {initial_capital:,} ì›)\n'
            title_text += f'Sharpe: {sharpe:.3f} | Sortino: {sortino:.3f} | MDD: {mdd:.2f}%'
            ax.set_title(title_text, fontsize=13, pad=15)
            
            # ì¶• ë ˆì´ë¸”
            ax.set_xlabel('ë‚ ì§œ', fontsize=11)
            ax.set_ylabel('í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜ (ì›)', fontsize=11)
            
            # ë²”ë¡€
            ax.legend(loc='upper left', fontsize=9, framealpha=0.95, 
                     fancybox=True, shadow=True)
            
            # ê·¸ë¦¬ë“œ
            ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)
            ax.set_axisbelow(True)
            
            # ë‚ ì§œ í¬ë§·
            if use_dates:
                ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))
                ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))
                plt.setp(ax.xaxis.get_majorticklabels(), rotation=0, ha='center')
            
            # yì¶• í¬ë§·
            ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x):,}'))
            
            # ìŠ¤íƒ€ì¼
            ax.spines['top'].set_visible(True)
            ax.spines['right'].set_visible(True)
            
            # ì—¬ë°±
            plt.tight_layout()
            
            # ì €ì¥
            plt.savefig('backtest_result.png', dpi=300, bbox_inches='tight', facecolor='white')
            print("    âœ… ê·¸ë˜í”„ ì €ì¥: backtest_result.png")
            plt.close()
            
            # ì„±ëŠ¥ ë¹„êµ ì¶œë ¥
            print(f"\n--- [3-1] Strategy Comparison ---")
            print(f"    {'Strategy':<20} {'Final Value':>18} {'Return':>10} {'vs KOSPI':>10}")
            print(f"    {'-'*65}")
            print(f"    {'QMIX Agent':<20} {ai_final:>18,.0f} {ai_return:>9.2f}% {ai_return - kospi_return:>9.2f}%")
            print(f"    {'Buy & Hold':<20} {bh_final:>18,.0f} {bh_return:>9.2f}% {bh_return - kospi_return:>9.2f}%")
            print(f"    {'KOSPI':<20} {kospi_final:>18,.0f} {kospi_return:>9.2f}% {0:>9.2f}%")
            print(f"\n    Performance Metrics:")
            print(f"    - Sharpe Ratio: {sharpe:.3f}")
            print(f"    - Sortino Ratio: {sortino:.3f}")
            print(f"    - Max Drawdown: {mdd:.2f}%")
            
        except Exception as e:
            print(f"    âš ï¸  ê·¸ë˜í”„ ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()

    # --- ìµœì¢…ì¼ ë¶„ì„ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€) ---
    print("\n--- [4] ìµœì¢…ì¼ ì˜ˆì¸¡ ìƒì„¸ ë¶„ì„ ---")
    
    final_obs_dict = obs_dict
    action_map = {0: "Long", 1: "Hold", 2: "Short"}
    action_indices = list(action_map.keys())
    action_names = list(action_map.values())
    
    obs_tensors = [torch.FloatTensor(final_obs_dict[f'agent_{i}']).unsqueeze(0).to(DEVICE) for i in range(N_AGENTS)]
    state_tensor = torch.FloatTensor(global_state).unsqueeze(0).to(DEVICE)
    
    q_vals_all_agents = []
    with torch.no_grad():
        for i, agent in enumerate(learner.agents):
            q_vals_all_agents.append(agent.get_q_values(obs_tensors[i]))

    agent_q_inputs = []
    action_tuples = []
    
    q_vals_0 = q_vals_all_agents[0].squeeze(0)
    q_vals_1 = q_vals_all_agents[1].squeeze(0)
    q_vals_2 = q_vals_all_agents[2].squeeze(0)

    for i, a0_idx in enumerate(action_indices):
        for j, a1_idx in enumerate(action_indices):
            for k, a2_idx in enumerate(action_indices): 
                q0 = q_vals_0[a0_idx]
                q1 = q_vals_1[a1_idx]
                q2 = q_vals_2[a2_idx]
                agent_q_inputs.append(torch.stack([q0, q1, q2])) 
                action_tuples.append((a0_idx, a1_idx, a2_idx))
    
    agent_q_batch = torch.stack(agent_q_inputs) 
    state_batch = state_tensor.repeat(len(action_tuples), 1)

    with torch.no_grad():
        all_q_totals = learner.mixer(agent_q_batch, state_batch)
    
    q_total_grid = all_q_totals.view(
        len(action_indices), len(action_indices), len(action_indices) 
    ).cpu().numpy()
    
    best_q_total_value = all_q_totals.max().item()
    best_joint_action_idx_flat = all_q_totals.argmax().item()
    best_joint_action_indices = action_tuples[best_joint_action_idx_flat]
    
    agent_analyses = []
    feature_names_list = [agent_0_cols, agent_1_cols, agent_2_cols] 
    n_features_list = [
        train_env.n_features_agent_0, 
        train_env.n_features_agent_1, 
        train_env.n_features_agent_2
    ]
    
    for i, agent in enumerate(learner.agents):
        obs = final_obs_dict[f'agent_{i}']
        agent_feature_names = feature_names_list[i]
        n_features_agent = n_features_list[i]

        action_idx, q_values, importance = agent.get_prediction_with_reason(
            obs, 
            agent_feature_names,
            WINDOW_SIZE, 
            n_features_agent
        )
        agent_analyses.append((action_idx, q_values, importance))
        
    final_signal = convert_joint_action_to_signal(best_joint_action_indices, action_map)
    ai_explanation = generate_ai_explanation(final_signal, agent_analyses)
    
    current_indicator_values = test_features_unnorm.iloc[-1]
    
    print_ui_output(
        final_signal=final_signal,
        ai_explanation=ai_explanation,
        current_indicators=current_indicator_values,
        q_total_grid=q_total_grid,
        best_q_total_value=best_q_total_value,
        action_names=action_names
    )
    
    end_time = time.time()
    total_time = end_time - start_time
    print("\n=============================================")
    print(f"  [ ğŸ“Š ì´ ì‹¤í–‰ ì‹œê°„ ]")
    print(f"    {total_time // 60:.0f} ë¶„ {total_time % 60:.2f} ì´ˆ")
    print("=============================================")


if __name__ == "__main__":
    main()